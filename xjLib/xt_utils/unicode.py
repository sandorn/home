# !/usr/bin/env python
"""
==============================================================
Description  : Unicodeå·¥å…·æ¨¡å— - ç²¾ç¡®å¤„ç† Unicode æ–‡æœ¬,æä¾›Unicodeå­—ç¬¦åˆ†æã€æ–‡æœ¬è§„èŒƒåŒ–å’Œæ¸…ç†åŠŸèƒ½
Develop      : VSCode
Author       : sandorn sandorn@live.cn
Date         : 2025-06-16 13:20:56
LastEditTime : 2025-09-17 14:30:00
FilePath     : /CODE/xjlib/xt_utils/unicode.py
Github       : https://github.com/sandorn/home

æœ¬æ¨¡å—æä¾›ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½:
- Unicodeå­—ç¬¦å±æ€§åˆ†æä¸åˆ†ç±»
- æ–‡æœ¬è§„èŒƒåŒ–å¤„ç†(NFCã€NFDã€NFKCã€NFKD)
- é«˜çº§æ–‡æœ¬æ¸…ç†åŠŸèƒ½(æ§åˆ¶å­—ç¬¦ç§»é™¤ã€é‡éŸ³ç¬¦å·å¤„ç†ã€ASCIIè½¬æ¢)
- å­—ç¬¦ç±»åˆ«è¿‡æ»¤

ä¸»è¦ç‰¹æ€§:
- å…¨é¢çš„Unicodeå­—ç¬¦å±æ€§æŸ¥è¯¢(åç§°ã€ç±»åˆ«ã€æ•°å€¼ç­‰)
- æ”¯æŒå¤šç§Unicodeè§„èŒƒåŒ–å½¢å¼æ¯”è¾ƒ
- ç»†ç²’åº¦çš„æ–‡æœ¬æ¸…ç†æ§åˆ¶é€‰é¡¹
- æ¸…ç†æ•ˆæœåˆ†æå’Œç»Ÿè®¡
- å®Œæ•´çš„ç±»å‹æ³¨è§£æ”¯æŒ
- ä¸xjlibå…¶ä»–æ¨¡å—ä¸€è‡´çš„APIè®¾è®¡å’Œæ–‡æ¡£é£æ ¼
==============================================================
"""

from __future__ import annotations

import re
import unicodedata
from collections.abc import Mapping
from typing import Any, TypeVar

T = TypeVar('T')


def analyze_character_properties(text: str) -> list[dict[str, Any]]:
    """
    åˆ†ææ–‡æœ¬ä¸­æ¯ä¸ªå­—ç¬¦çš„åŸºæœ¬Unicodeå±æ€§

    Args:
        text: è¦åˆ†æçš„æ–‡æœ¬å­—ç¬¦ä¸²

    Returns:
        List[Dict[str, Any]]: åŒ…å«æ¯ä¸ªå­—ç¬¦å±æ€§ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
            - character: åŸå§‹å­—ç¬¦
            - name: Unicodeå­—ç¬¦åç§°
            - category: Unicodeå­—ç¬¦åˆ†ç±»
            - numeric_value: æ•°å­—å€¼(å¦‚æœé€‚ç”¨)
            - decimal_value: åè¿›åˆ¶å€¼(å¦‚æœé€‚ç”¨)
            - digit_value: æ•°å­—å€¼(å¦‚æœé€‚ç”¨)

    Example:
        >>> results = analyze_character_properties('Hello123')
        >>> for result in results:
        >>>     print(f"å­—ç¬¦: '{result['character']}' - åç§°: {result['name']} - åˆ†ç±»: {result['category']}")
    """
    analysis_results: list[dict[str, Any]] = []

    for char in text:
        char_info = {
            'character': char,
            'name': unicodedata.name(char, 'UNKNOWN'),
            'category': unicodedata.category(char),
            'numeric_value': unicodedata.numeric(char, None),
            'decimal_value': unicodedata.decimal(char, None),
            'digit_value': unicodedata.digit(char, None),
        }
        analysis_results.append(char_info)

    return analysis_results


class UnicodeCharacterAnalyzer:
    """Unicodeå­—ç¬¦åˆ†æå™¨ - æä¾›å­—ç¬¦åˆ†ç±»ç»Ÿè®¡å’Œè¿‡æ»¤åŠŸèƒ½"""

    def __init__(self) -> None:
        """åˆå§‹åŒ–å­—ç¬¦åˆ†æå™¨ï¼ŒåŠ è½½Unicodeå­—ç¬¦åˆ†ç±»æè¿°æ˜ å°„"""
        self.category_descriptions: Mapping[str, str] = {
            'Lu': 'å¤§å†™å­—æ¯',
            'Ll': 'å°å†™å­—æ¯',
            'Lt': 'è¯é¦–å¤§å†™å­—æ¯',
            'Lm': 'ä¿®é¥°å­—æ¯',
            'Lo': 'å…¶ä»–å­—æ¯',
            'Mn': 'éç©ºæ ¼æ ‡è®°',
            'Mc': 'ç©ºæ ¼ç»„åˆæ ‡è®°',
            'Me': 'å°é—­æ ‡è®°',
            'Nd': 'åè¿›åˆ¶æ•°å­—',
            'Nl': 'å­—æ¯æ•°å­—',
            'No': 'å…¶ä»–æ•°å­—',
            'Pc': 'è¿æ¥æ ‡ç‚¹',
            'Pd': 'æ¨ªçº¿æ ‡ç‚¹',
            'Ps': 'å¼€æ”¾æ ‡ç‚¹',
            'Pe': 'å…³é—­æ ‡ç‚¹',
            'Pi': 'åˆå§‹æ ‡ç‚¹',
            'Pf': 'æœ€ç»ˆæ ‡ç‚¹',
            'Po': 'å…¶ä»–æ ‡ç‚¹',
            'Sm': 'æ•°å­¦ç¬¦å·',
            'Sc': 'è´§å¸ç¬¦å·',
            'Sk': 'ä¿®é¥°ç¬¦å·',
            'So': 'å…¶ä»–ç¬¦å·',
            'Zs': 'ç©ºæ ¼åˆ†éš”ç¬¦',
            'Zl': 'è¡Œåˆ†éš”ç¬¦',
            'Zp': 'æ®µè½åˆ†éš”ç¬¦',
            'Cc': 'æ§åˆ¶å­—ç¬¦',
            'Cf': 'æ ¼å¼å­—ç¬¦',
            'Cs': 'ä»£ç†å­—ç¬¦',
            'Co': 'ç§ç”¨å­—ç¬¦',
            'Cn': 'æœªåˆ†é…å­—ç¬¦',
        }

    def categorize_text(self, text: str) -> tuple[dict[str, int], list[dict[str, str]]]:
        """
        æŒ‰Unicodeç±»åˆ«ç»Ÿè®¡æ–‡æœ¬ä¸­çš„å­—ç¬¦åˆ†å¸ƒå¹¶æä¾›è¯¦ç»†åˆ†æ

        Args:
            text: è¦åˆ†ç±»çš„æ–‡æœ¬å­—ç¬¦ä¸²

        Returns:
            Tuple[Dict[str, int], List[Dict[str, str]]]:
                - ç¬¬ä¸€ä¸ªå…ƒç´ : å­—ç¬¦ç±»åˆ«åˆ°å‡ºç°æ¬¡æ•°çš„æ˜ å°„
                - ç¬¬äºŒä¸ªå…ƒç´ : æ¯ä¸ªå­—ç¬¦çš„è¯¦ç»†åˆ†æä¿¡æ¯åˆ—è¡¨

        Example:
            >>> analyzer = UnicodeCharacterAnalyzer()
            >>> category_counts, detailed_info = analyzer.categorize_text('Hello, ä¸–ç•Œï¼123')
            >>> # è¾“å‡ºç±»åˆ«ç»Ÿè®¡
            >>> for category, count in sorted(category_counts.items()):
            >>>     print(f"ç±»åˆ« {category}: {count}ä¸ªå­—ç¬¦")
        """
        category_counts: dict[str, int] = {}
        detailed_analysis: list[dict[str, str]] = []

        for char in text:
            category = unicodedata.category(char)
            category_counts[category] = category_counts.get(category, 0) + 1

            detailed_analysis.append({
                'char': char,
                'category': category,
                'description': self.category_descriptions.get(category, 'æœªçŸ¥åˆ†ç±»'),
                'name': unicodedata.name(char, f'U+{ord(char):04X}'),
            })

        return category_counts, detailed_analysis

    def filter_by_category(self, text: str, target_categories: list[str]) -> str:
        """
        æ ¹æ®Unicodeå­—ç¬¦ç±»åˆ«è¿‡æ»¤æ–‡æœ¬ï¼Œåªä¿ç•™æŒ‡å®šç±»åˆ«çš„å­—ç¬¦

        Args:
            text: è¦è¿‡æ»¤çš„æ–‡æœ¬å­—ç¬¦ä¸²
            target_categories: è¦ä¿ç•™çš„å­—ç¬¦ç±»åˆ«åˆ—è¡¨

        Returns:
            str: è¿‡æ»¤åçš„æ–‡æœ¬å­—ç¬¦ä¸²

        Example:
            >>> analyzer = UnicodeCharacterAnalyzer()
            >>> # åªä¿ç•™å­—æ¯å’Œæ•°å­—
            >>> filtered_text = analyzer.filter_by_category('Hello, ä¸–ç•Œï¼123', ['Lu', 'Ll', 'Lo', 'Nd'])
            >>> print(filtered_text)  # è¾“å‡º: Helloä¸–ç•Œ123
        """
        filtered_chars: list[str] = []
        for char in text:
            if unicodedata.category(char) in target_categories:
                filtered_chars.append(char)
        return ''.join(filtered_chars)


class AdvancedTextCleaner:
    """é«˜çº§æ–‡æœ¬æ¸…ç†å™¨ - æä¾›å…¨é¢çš„æ–‡æœ¬è§„èŒƒåŒ–å’Œæ¸…ç†åŠŸèƒ½"""

    def __init__(self) -> None:
        """åˆå§‹åŒ–æ–‡æœ¬æ¸…ç†å™¨ï¼Œè®¾ç½®é»˜è®¤æ¸…ç†å‚æ•°"""
        self.unwanted_categories: list[str] = ['Cc', 'Cf', 'Cs', 'Co', 'Cn']  # æ§åˆ¶å­—ç¬¦ç­‰

    def clean_and_normalize(
        self,
        text: str,
        normalize_form: str = 'NFKC',
        remove_accents: bool = True,
        keep_only_ascii: bool = False,
    ) -> str:
        """
        å…¨é¢çš„æ–‡æœ¬æ¸…ç†å’Œè§„èŒƒåŒ–å¤„ç†

        Args:
            text: è¦æ¸…ç†çš„æ–‡æœ¬å­—ç¬¦ä¸²
            normalize_form: Unicodeè§„èŒƒåŒ–å½¢å¼(NFCã€NFDã€NFKCã€NFKD)
            remove_accents: æ˜¯å¦ç§»é™¤é‡éŸ³ç¬¦å·
            keep_only_ascii: æ˜¯å¦åªä¿ç•™ASCIIå­—ç¬¦

        Returns:
            str: æ¸…ç†åçš„æ–‡æœ¬å­—ç¬¦ä¸²

        Example:
            >>> cleaner = AdvancedTextCleaner()
            >>> text = '  HÃ©llo   WÃ¶rld!  \t\n  '
            >>> # åŸºç¡€æ¸…ç†
            >>> basic_clean = cleaner.clean_and_normalize(text)
            >>> # ç§»é™¤é‡éŸ³
            >>> no_accents = cleaner.clean_and_normalize(text, remove_accents=True)
            >>> # ä»…ä¿ç•™ASCII
            >>> ascii_only = cleaner.clean_and_normalize(text, keep_only_ascii=True)
        """
        # 1. Unicodeè§„èŒƒåŒ–
        cleaned_text = unicodedata.normalize(normalize_form, text)

        # 2. ç§»é™¤ä¸éœ€è¦çš„æ§åˆ¶å­—ç¬¦
        cleaned_text = self._remove_unwanted_chars(cleaned_text)

        # 3. å¯é€‰ï¼šç§»é™¤é‡éŸ³ç¬¦å·
        if remove_accents:
            cleaned_text = self._remove_accents(cleaned_text)

        # 4. å¯é€‰ï¼šåªä¿ç•™ASCIIå­—ç¬¦
        if keep_only_ascii:
            cleaned_text = self._keep_ascii_only(cleaned_text)

        # 5. è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
        return self._normalize_whitespace(cleaned_text)

    def _remove_unwanted_chars(self, text: str) -> str:
        """ç§»é™¤ä¸éœ€è¦çš„Unicodeå­—ç¬¦ç±»åˆ«(å†…éƒ¨æ–¹æ³•)"""
        return ''.join(char for char in text if unicodedata.category(char) not in self.unwanted_categories)

    def _remove_accents(self, text: str) -> str:
        """ç§»é™¤é‡éŸ³ç¬¦å·ï¼Œä¿ç•™åŸºç¡€å­—ç¬¦(å†…éƒ¨æ–¹æ³•)"""
        # å…ˆåˆ†è§£ä¸ºåŸºç¡€å­—ç¬¦å’Œç»„åˆæ ‡è®°
        nfd_text = unicodedata.normalize('NFD', text)
        # ç§»é™¤ç»„åˆæ ‡è®°ï¼ˆé‡éŸ³ç¬¦å·ç­‰ï¼‰
        without_accents = ''.join(char for char in nfd_text if unicodedata.category(char) != 'Mn')
        # é‡æ–°ç»„åˆ
        return unicodedata.normalize('NFC', without_accents)

    def _keep_ascii_only(self, text: str) -> str:
        """åªä¿ç•™ASCIIå­—ç¬¦(å†…éƒ¨æ–¹æ³•)"""
        return ''.join(char for char in text if ord(char) < 128)

    def _normalize_whitespace(self, text: str) -> str:
        """è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦(å†…éƒ¨æ–¹æ³•)"""
        # å°†æ‰€æœ‰ç©ºç™½å­—ç¬¦æ›¿æ¢ä¸ºæ™®é€šç©ºæ ¼ï¼Œå¹¶å»é™¤å¤šä½™ç©ºæ ¼
        return re.sub(r'\s+', ' ', text).strip()

    def analyze_cleaning_impact(self, original_text: str, cleaned_text: str) -> dict[str, int | float]:
        """
        åˆ†ææ–‡æœ¬æ¸…ç†çš„æ•ˆæœå’Œå½±å“

        Args:
            original_text: åŸå§‹æ–‡æœ¬
            cleaned_text: æ¸…ç†åçš„æ–‡æœ¬

        Returns:
            Dict[str, Union[int, float]]: åŒ…å«æ¸…ç†æ•ˆæœç»Ÿè®¡çš„å­—å…¸
                - original_length: åŸå§‹å­—ç¬¦é•¿åº¦
                - cleaned_length: æ¸…ç†åå­—ç¬¦é•¿åº¦
                - removed_chars: ç§»é™¤çš„å­—ç¬¦æ•°é‡
                - reduction_percentage: å­—ç¬¦å‡å°‘ç™¾åˆ†æ¯”
                - original_bytes: åŸå§‹å­—èŠ‚å¤§å°
                - cleaned_bytes: æ¸…ç†åå­—èŠ‚å¤§å°

        Example:
            >>> cleaner = AdvancedTextCleaner()
            >>> original = '  HÃ©llo   WÃ¶rld!  \t\n  '
            >>> cleaned = cleaner.clean_and_normalize(original)
            >>> impact = cleaner.analyze_cleaning_impact(original, cleaned)
            >>> print(f'å­—ç¬¦å‡å°‘: {impact["removed_chars"]}ä¸ª ({impact["reduction_percentage"]:.1f}%)')
        """
        original_len = len(original_text)
        return {
            'original_length': original_len,
            'cleaned_length': len(cleaned_text),
            'removed_chars': original_len - len(cleaned_text),
            'reduction_percentage': ((original_len - len(cleaned_text)) / original_len * 100) if original_len else 0,
            'original_bytes': len(original_text.encode('utf-8')),
            'cleaned_bytes': len(cleaned_text.encode('utf-8')),
        }


class TextNormalizer:
    """æ–‡æœ¬è§„èŒƒåŒ–å¤„ç†å™¨ - æä¾›Unicodeè§„èŒƒåŒ–å’Œæ¯”è¾ƒåŠŸèƒ½"""

    def __init__(self) -> None:
        """åˆå§‹åŒ–æ–‡æœ¬è§„èŒƒåŒ–å™¨ï¼Œè®¾ç½®æ”¯æŒçš„è§„èŒƒåŒ–å½¢å¼"""
        self.normalization_forms: list[str] = ['NFC', 'NFD', 'NFKC', 'NFKD']
        self.form_descriptions: Mapping[str, str] = {
            'NFC': 'è§„èŒƒç»„åˆå½¢å¼ (Canonical Composition)',
            'NFD': 'è§„èŒƒåˆ†è§£å½¢å¼ (Canonical Decomposition)',
            'NFKC': 'å…¼å®¹ç»„åˆå½¢å¼ (Compatibility Composition)',
            'NFKD': 'å…¼å®¹åˆ†è§£å½¢å¼ (Compatibility Decomposition)',
        }

    def normalize_text(self, text: str, form: str = 'NFC') -> str:
        """
        ä½¿ç”¨æŒ‡å®šçš„Unicodeè§„èŒƒåŒ–å½¢å¼å¤„ç†æ–‡æœ¬

        Args:
            text: è¦è§„èŒƒåŒ–çš„æ–‡æœ¬
            form: è§„èŒƒåŒ–å½¢å¼(NFCã€NFDã€NFKCã€NFKD)

        Returns:
            str: è§„èŒƒåŒ–åçš„æ–‡æœ¬

        Raises:
            ValueError: å½“æŒ‡å®šäº†ä¸æ”¯æŒçš„è§„èŒƒåŒ–å½¢å¼æ—¶

        Example:
            >>> normalizer = TextNormalizer()
            >>> text = 'cafÃ©'  # é¢„ç»„åˆçš„Ã©
            >>> normalized = normalizer.normalize_text(text, form='NFD')
        """
        if form not in self.normalization_forms:
            raise ValueError(f'ä¸æ”¯æŒçš„è§„èŒƒåŒ–å½¢å¼: {form}')

        return unicodedata.normalize(form, text)

    def compare_normalizations(self, text: str) -> dict[str, dict[str, str | int]]:
        """
        æ¯”è¾ƒä¸åŒUnicodeè§„èŒƒåŒ–å½¢å¼å¯¹æ–‡æœ¬çš„å½±å“

        Args:
            text: è¦æ¯”è¾ƒçš„æ–‡æœ¬

        Returns:
            Dict[str, Dict[str, Union[str, int]]]: ä¸åŒè§„èŒƒåŒ–å½¢å¼çš„ç»“æœæ¯”è¾ƒ
                - é”®: è§„èŒƒåŒ–å½¢å¼(NFCã€NFDã€NFKCã€NFKD)
                - å€¼: åŒ…å«è§„èŒƒåŒ–åæ–‡æœ¬ã€é•¿åº¦ã€å­—èŠ‚æ•°å’Œæè¿°çš„å­—å…¸

        Example:
            >>> normalizer = TextNormalizer()
            >>> comparisons = normalizer.compare_normalizations('cafÃ©')
            >>> for form, result in comparisons.items():
            >>>     print(f"{form}: é•¿åº¦={result['length']}, å­—èŠ‚æ•°={result['bytes']}")
        """
        results: dict[str, dict[str, str | int]] = {}

        for form in self.normalization_forms:
            normalized = unicodedata.normalize(form, text)
            results[form] = {
                'text': normalized,
                'length': len(normalized),
                'bytes': len(normalized.encode('utf-8')),
                'description': self.form_descriptions[form],
            }

        return results

    def is_normalized(self, text: str, form: str = 'NFC') -> bool:
        """
        æ£€æŸ¥æ–‡æœ¬æ˜¯å¦å·²ç»æ˜¯æŒ‡å®šçš„Unicodeè§„èŒƒåŒ–å½¢å¼

        Args:
            text: è¦æ£€æŸ¥çš„æ–‡æœ¬
            form: è§„èŒƒåŒ–å½¢å¼(NFCã€NFDã€NFKCã€NFKD)

        Returns:
            bool: å¦‚æœæ–‡æœ¬å·²ç»æ˜¯æŒ‡å®šçš„è§„èŒƒåŒ–å½¢å¼åˆ™è¿”å›True

        Example:
            >>> normalizer = TextNormalizer()
            >>> text = 'cafÃ©'  # é¢„ç»„åˆçš„Ã©
            >>> is_normal = normalizer.is_normalized(text, form='NFC')  # è¿”å›True
        """
        normalized = unicodedata.normalize(form, text)
        return text == normalized


# åˆ›å»ºå…¨å±€å®ä¾‹ï¼Œæ–¹ä¾¿ç›´æ¥ä½¿ç”¨
unicode_analyzer = UnicodeCharacterAnalyzer()
text_cleaner = AdvancedTextCleaner()
text_normalizer = TextNormalizer()


def get_character_name(char: str, default: str = 'UNKNOWN') -> str:
    """
    è·å–å•ä¸ªå­—ç¬¦çš„Unicodeåç§°

    Args:
        char: è¦æŸ¥è¯¢çš„å­—ç¬¦
        default: æ— æ³•è·å–åç§°æ—¶çš„é»˜è®¤å€¼

    Returns:
        str: Unicodeå­—ç¬¦åç§°æˆ–é»˜è®¤å€¼

    Example:
        >>> get_character_name('A')  # è¿”å›: 'LATIN CAPITAL LETTER A'
        >>> get_character_name('ä¸–')  # è¿”å›: 'CJK UNIFIED IDEOGRAPH-4E16'
    """
    return unicodedata.name(char, default)


def get_character_category(char: str) -> str:
    """
    è·å–å•ä¸ªå­—ç¬¦çš„Unicodeç±»åˆ«

    Args:
        char: è¦æŸ¥è¯¢çš„å­—ç¬¦

    Returns:
        str: Unicodeå­—ç¬¦ç±»åˆ«ä»£ç 

    Example:
        >>> get_character_category('A')  # è¿”å›: 'Lu'
        >>> get_character_category('1')  # è¿”å›: 'Nd'
    """
    return unicodedata.category(char)


def get_category_description(category: str) -> str:
    """
    è·å–Unicodeå­—ç¬¦ç±»åˆ«çš„ä¸­æ–‡æè¿°

    Args:
        category: Unicodeå­—ç¬¦ç±»åˆ«ä»£ç 

    Returns:
        str: ç±»åˆ«æè¿°æ–‡æœ¬

    Example:
        >>> get_category_description('Lu')  # è¿”å›: 'å¤§å†™å­—æ¯'
        >>> get_category_description('Nd')  # è¿”å›: 'åè¿›åˆ¶æ•°å­—'
    """
    analyzer = UnicodeCharacterAnalyzer()
    return analyzer.category_descriptions.get(category, 'æœªçŸ¥åˆ†ç±»')


def is_control_character(char: str) -> bool:
    """
    æ£€æŸ¥å­—ç¬¦æ˜¯å¦ä¸ºæ§åˆ¶å­—ç¬¦

    Args:
        char: è¦æ£€æŸ¥çš„å­—ç¬¦

    Returns:
        bool: å¦‚æœæ˜¯æ§åˆ¶å­—ç¬¦åˆ™è¿”å›True

    Example:
        >>> is_control_character('\n')  # è¿”å›: True
        >>> is_control_character('A')  # è¿”å›: False
    """
    return unicodedata.category(char).startswith('C')


def normalize_whitespace(text: str) -> str:
    """
    è§„èŒƒåŒ–æ–‡æœ¬ä¸­çš„ç©ºç™½å­—ç¬¦

    Args:
        text: è¦å¤„ç†çš„æ–‡æœ¬

    Returns:
        str: è§„èŒƒåŒ–åçš„æ–‡æœ¬

    Example:
        >>> normalize_whitespace('  Hello   \t\nWorld  ')  # è¿”å›: 'Hello World'
    """
    return re.sub(r'\s+', ' ', text).strip()


if __name__ == '__main__':
    # ä½¿ç”¨é«˜çº§æ–‡æœ¬æ¸…ç†å™¨
    cleaner = AdvancedTextCleaner()

    # æµ‹è¯•å„ç§å¤æ‚æ–‡æœ¬
    test_cases = [
        '  HÃ©llo   WÃ¶rld!  \t\n  ',  # é‡éŸ³ç¬¦å·å’Œå¤šä½™ç©ºç™½
        'cafÃ©ï¼ƒâ‘ â‘¡â‘¢',  # æ··åˆå­—ç¬¦
        'æ–‡ä»¶åï¼šrÃ©sumÃ©.txt',  # ä¸­è‹±æ··åˆå¸¦é‡éŸ³
    ]

    for i, test_text in enumerate(test_cases, 1):
        print(f'\n=== æµ‹è¯•æ¡ˆä¾‹ {i} ===')
        print(f"åŸæ–‡: '{test_text}'")

        # ä¸åŒçº§åˆ«çš„æ¸…ç†
        basic_clean = cleaner.clean_and_normalize(test_text)
        no_accents = cleaner.clean_and_normalize(test_text, remove_accents=True)
        ascii_only = cleaner.clean_and_normalize(test_text, remove_accents=True, keep_only_ascii=True)

        print(f"åŸºç¡€æ¸…ç†: '{basic_clean}'")
        print(f"ç§»é™¤é‡éŸ³: '{no_accents}'")
        print(f"ä»…ASCII: '{ascii_only}'")

        # åˆ†ææ¸…ç†æ•ˆæœ
        impact = cleaner.analyze_cleaning_impact(test_text, ascii_only)
        print(f'æ¸…ç†æ•ˆæœ: å­—ç¬¦å‡å°‘ {impact["removed_chars"]} ä¸ª ({impact["reduction_percentage"]:.1f}%)')

    # æµ‹è¯•æ–‡æœ¬è§„èŒƒåŒ–
    normalizer = TextNormalizer()

    # åŒ…å«ç»„åˆå­—ç¬¦çš„æµ‹è¯•æ–‡æœ¬
    test_strings = [
        'cafÃ©',  # e + ç»„åˆé‡éŸ³ç¬¦
        'cafÃ©',  # é¢„ç»„åˆçš„Ã©
        'â‘ â‘¡â‘¢',  # åœ†åœˆæ•°å­—
        'ï¬le',  # è¿å­—ç¬¦
    ]

    for test_str in test_strings:
        print(f"\nåŸå§‹æ–‡æœ¬: '{test_str}' (é•¿åº¦: {len(test_str)})")

        comparison = normalizer.compare_normalizations(test_str)

        for form, result in comparison.items():
            equal_mark = 'âœ“' if result['text'] == test_str else 'âœ—'
            print(f"  {form}: '{result['text']}' (é•¿åº¦: {result['length']}, å­—èŠ‚: {result['bytes']}) {equal_mark}")

    # æµ‹è¯•ä¸åŒç±»å‹çš„å­—ç¬¦
    test_text = 'Helloä¸–ç•Œ123ï¼'
    results = analyze_character_properties(test_text)

    for result in results:
        print(f"å­—ç¬¦: '{result['character']}' - åç§°: {result['name']}")
        print(f'  åˆ†ç±»: {result["category"]}')
        if result['numeric_value'] is not None:
            print(f'  æ•°å€¼: {result["numeric_value"]}')
        print('---')

    # ä½¿ç”¨å­—ç¬¦åˆ†æå™¨
    analyzer = UnicodeCharacterAnalyzer()

    sample_text = 'Hello, ä¸–ç•Œï¼123 $100 Î±+Î²=Î³ ğŸš€'
    category_counts, detailed_info = analyzer.categorize_text(sample_text)

    print('å­—ç¬¦åˆ†ç±»ç»Ÿè®¡:')
    for category, count in sorted(category_counts.items()):
        description = analyzer.category_descriptions.get(category, 'æœªçŸ¥')
        print(f'  {category} ({description}): {count}ä¸ª')

    print('\nä»…ä¿ç•™å­—æ¯å’Œæ•°å­—:')
    letters_and_digits = analyzer.filter_by_category(sample_text, ['Lu', 'Ll', 'Lo', 'Nd'])
    print(f'åŸæ–‡: {sample_text}')
    print(f'è¿‡æ»¤å: {letters_and_digits}')
