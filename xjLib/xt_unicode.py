# !/usr/bin/env python
"""
==============================================================
Description  : å¤´éƒ¨æ³¨é‡Š
Develop      : VSCode
Author       : sandorn sandorn@live.cn
Date         : 2025-06-16 13:20:56
LastEditTime : 2025-06-16 14:11:31
FilePath     : /CODE/xjLib/xt_unicodedata.py
Github       : https://github.com/sandorn/home
==============================================================
"""

import re
import unicodedata


def analyze_character_properties(text):
    """åˆ†ææ–‡æœ¬ä¸­æ¯ä¸ªå­—ç¬¦çš„åŸºæœ¬å±æ€§"""
    analysis_results = []

    for char in text:
        char_info = {
            "character": char,
            "name": unicodedata.name(char, "UNKNOWN"),
            "category": unicodedata.category(char),
            "numeric_value": unicodedata.numeric(char, None),
            "decimal_value": unicodedata.decimal(char, None),
            "digit_value": unicodedata.digit(char, None),
        }
        analysis_results.append(char_info)

    return analysis_results


class UnicodeCharacterAnalyzer:
    """Unicodeå­—ç¬¦åˆ†æå™¨"""

    def __init__(self):
        self.category_descriptions = {
            "Lu": "å¤§å†™å­—æ¯",
            "Ll": "å°å†™å­—æ¯",
            "Lt": "è¯é¦–å¤§å†™å­—æ¯",
            "Lm": "ä¿®é¥°å­—æ¯",
            "Lo": "å…¶ä»–å­—æ¯",
            "Mn": "éç©ºæ ¼æ ‡è®°",
            "Mc": "ç©ºæ ¼ç»„åˆæ ‡è®°",
            "Me": "å°é—­æ ‡è®°",
            "Nd": "åè¿›åˆ¶æ•°å­—",
            "Nl": "å­—æ¯æ•°å­—",
            "No": "å…¶ä»–æ•°å­—",
            "Pc": "è¿æ¥æ ‡ç‚¹",
            "Pd": "æ¨ªçº¿æ ‡ç‚¹",
            "Ps": "å¼€æ”¾æ ‡ç‚¹",
            "Pe": "å…³é—­æ ‡ç‚¹",
            "Pi": "åˆå§‹æ ‡ç‚¹",
            "Pf": "æœ€ç»ˆæ ‡ç‚¹",
            "Po": "å…¶ä»–æ ‡ç‚¹",
            "Sm": "æ•°å­¦ç¬¦å·",
            "Sc": "è´§å¸ç¬¦å·",
            "Sk": "ä¿®é¥°ç¬¦å·",
            "So": "å…¶ä»–ç¬¦å·",
            "Zs": "ç©ºæ ¼åˆ†éš”ç¬¦",
            "Zl": "è¡Œåˆ†éš”ç¬¦",
            "Zp": "æ®µè½åˆ†éš”ç¬¦",
            "Cc": "æ§åˆ¶å­—ç¬¦",
            "Cf": "æ ¼å¼å­—ç¬¦",
            "Cs": "ä»£ç†å­—ç¬¦",
            "Co": "ç§ç”¨å­—ç¬¦",
            "Cn": "æœªåˆ†é…å­—ç¬¦",
        }

    def categorize_text(self, text):
        """æŒ‰ç±»åˆ«ç»Ÿè®¡æ–‡æœ¬ä¸­çš„å­—ç¬¦"""
        category_counts = {}
        detailed_analysis = []

        for char in text:
            category = unicodedata.category(char)
            category_counts[category] = category_counts.get(category, 0) + 1

            detailed_analysis.append(
                {
                    "char": char,
                    "category": category,
                    "description": self.category_descriptions.get(category, "æœªçŸ¥åˆ†ç±»"),
                    "name": unicodedata.name(char, f"U+{ord(char):04X}"),
                }
            )

        return category_counts, detailed_analysis

    def filter_by_category(self, text, target_categories):
        """æ ¹æ®å­—ç¬¦ç±»åˆ«è¿‡æ»¤æ–‡æœ¬"""
        filtered_chars = []
        for char in text:
            if unicodedata.category(char) in target_categories:
                filtered_chars.append(char)
        return "".join(filtered_chars)


class AdvancedTextCleaner:
    """é«˜çº§æ–‡æœ¬æ¸…ç†å™¨"""

    def __init__(self):
        self.unwanted_categories = ["Cc", "Cf", "Cs", "Co", "Cn"]  # æ§åˆ¶å­—ç¬¦ç­‰

    def clean_and_normalize(
        self, text, normalize_form="NFKC", remove_accents=True, keep_only_ascii=False
    ):
        """å…¨é¢çš„æ–‡æœ¬æ¸…ç†å’Œè§„èŒƒåŒ–"""

        # 1. Unicodeè§„èŒƒåŒ–
        cleaned_text = unicodedata.normalize(normalize_form, text)

        # 2. ç§»é™¤ä¸éœ€è¦çš„æ§åˆ¶å­—ç¬¦
        cleaned_text = self._remove_unwanted_chars(cleaned_text)

        # 3. å¯é€‰ï¼šç§»é™¤é‡éŸ³ç¬¦å·
        if remove_accents:
            cleaned_text = self._remove_accents(cleaned_text)

        # 4. å¯é€‰ï¼šåªä¿ç•™ASCIIå­—ç¬¦
        if keep_only_ascii:
            cleaned_text = self._keep_ascii_only(cleaned_text)

        # 5. è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
        cleaned_text = self._normalize_whitespace(cleaned_text)

        return cleaned_text

    def _remove_unwanted_chars(self, text):
        """ç§»é™¤ä¸éœ€è¦çš„Unicodeå­—ç¬¦ç±»åˆ«"""
        return "".join(
            char
            for char in text
            if unicodedata.category(char) not in self.unwanted_categories
        )

    def _remove_accents(self, text):
        """ç§»é™¤é‡éŸ³ç¬¦å·ï¼Œä¿ç•™åŸºç¡€å­—ç¬¦"""
        # å…ˆåˆ†è§£ä¸ºåŸºç¡€å­—ç¬¦å’Œç»„åˆæ ‡è®°
        nfd_text = unicodedata.normalize("NFD", text)
        # ç§»é™¤ç»„åˆæ ‡è®°ï¼ˆé‡éŸ³ç¬¦å·ç­‰ï¼‰
        without_accents = "".join(
            char for char in nfd_text if unicodedata.category(char) != "Mn"
        )
        # é‡æ–°ç»„åˆ
        return unicodedata.normalize("NFC", without_accents)

    def _keep_ascii_only(self, text):
        """åªä¿ç•™ASCIIå­—ç¬¦"""
        return "".join(char for char in text if ord(char) < 128)

    def _normalize_whitespace(self, text):
        """è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦"""
        # å°†æ‰€æœ‰ç©ºç™½å­—ç¬¦æ›¿æ¢ä¸ºæ™®é€šç©ºæ ¼ï¼Œå¹¶å»é™¤å¤šä½™ç©ºæ ¼
        return re.sub(r"\s+", " ", text).strip()

    def analyze_cleaning_impact(self, original_text, cleaned_text):
        """åˆ†ææ¸…ç†æ•ˆæœ"""
        return {
            "original_length": len(original_text),
            "cleaned_length": len(cleaned_text),
            "removed_chars": len(original_text) - len(cleaned_text),
            "reduction_percentage": (
                (len(original_text) - len(cleaned_text)) / len(original_text) * 100
            )
            if original_text
            else 0,
            "original_bytes": len(original_text.encode("utf-8")),
            "cleaned_bytes": len(cleaned_text.encode("utf-8")),
        }


class TextNormalizer:
    """æ–‡æœ¬è§„èŒƒåŒ–å¤„ç†å™¨"""

    def __init__(self):
        self.normalization_forms = ["NFC", "NFD", "NFKC", "NFKD"]
        self.form_descriptions = {
            "NFC": "è§„èŒƒç»„åˆå½¢å¼ (Canonical Composition)",
            "NFD": "è§„èŒƒåˆ†è§£å½¢å¼ (Canonical Decomposition)",
            "NFKC": "å…¼å®¹ç»„åˆå½¢å¼ (Compatibility Composition)",
            "NFKD": "å…¼å®¹åˆ†è§£å½¢å¼ (Compatibility Decomposition)",
        }

    def normalize_text(self, text, form="NFC"):
        """ä½¿ç”¨æŒ‡å®šå½¢å¼è§„èŒƒåŒ–æ–‡æœ¬"""
        if form not in self.normalization_forms:
            raise ValueError(f"ä¸æ”¯æŒçš„è§„èŒƒåŒ–å½¢å¼: {form}")

        return unicodedata.normalize(form, text)

    def compare_normalizations(self, text):
        """æ¯”è¾ƒä¸åŒè§„èŒƒåŒ–å½¢å¼çš„ç»“æœ"""
        results = {}

        for form in self.normalization_forms:
            normalized = unicodedata.normalize(form, text)
            results[form] = {
                "text": normalized,
                "length": len(normalized),
                "bytes": len(normalized.encode("utf-8")),
                "description": self.form_descriptions[form],
            }

        return results

    def is_normalized(self, text, form="NFC"):
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦å·²ç»æ˜¯æŒ‡å®šçš„è§„èŒƒåŒ–å½¢å¼"""
        normalized = unicodedata.normalize(form, text)
        return text == normalized


if __name__ == "__main__":
    # ä½¿ç”¨é«˜çº§æ–‡æœ¬æ¸…ç†å™¨
    cleaner = AdvancedTextCleaner()

    # æµ‹è¯•å„ç§å¤æ‚æ–‡æœ¬
    test_cases = [
        "  HÃ©llo   WÃ¶rld!  \t\n  ",  # é‡éŸ³ç¬¦å·å’Œå¤šä½™ç©ºç™½
        "cafÃ©ï¼ƒâ‘ â‘¡â‘¢",  # æ··åˆå­—ç¬¦
        "æ–‡ä»¶åï¼šrÃ©sumÃ©.txt",  # ä¸­è‹±æ··åˆå¸¦é‡éŸ³
    ]

    for i, test_text in enumerate(test_cases, 1):
        print(f"\n=== æµ‹è¯•æ¡ˆä¾‹ {i} ===")
        print(f"åŸæ–‡: '{test_text}'")

        # ä¸åŒçº§åˆ«çš„æ¸…ç†
        basic_clean = cleaner.clean_and_normalize(test_text)
        no_accents = cleaner.clean_and_normalize(test_text, remove_accents=True)
        ascii_only = cleaner.clean_and_normalize(
            test_text, remove_accents=True, keep_only_ascii=True
        )

        print(f"åŸºç¡€æ¸…ç†: '{basic_clean}'")
        print(f"ç§»é™¤é‡éŸ³: '{no_accents}'")
        print(f"ä»…ASCII: '{ascii_only}'")

        # åˆ†ææ¸…ç†æ•ˆæœ
        impact = cleaner.analyze_cleaning_impact(test_text, ascii_only)
        print(
            f"æ¸…ç†æ•ˆæœ: å­—ç¬¦å‡å°‘ {impact['removed_chars']} ä¸ª ({impact['reduction_percentage']:.1f}%)"
        )

    # è¾“å‡ºç¤ºä¾‹:
    # === æµ‹è¯•æ¡ˆä¾‹ 1 ===
    # åŸæ–‡: '  HÃ©llo   WÃ¶rld!
    #   '
    # åŸºç¡€æ¸…ç†: 'HÃ©llo WÃ¶rld!'
    # ç§»é™¤é‡éŸ³: 'Hello World!'
    # ä»…ASCII: 'Hello World!'
    # æ¸…ç†æ•ˆæœ: å­—ç¬¦å‡å°‘ 12 ä¸ª (60.0%)

    # æµ‹è¯•æ–‡æœ¬è§„èŒƒåŒ–
    normalizer = TextNormalizer()

    # åŒ…å«ç»„åˆå­—ç¬¦çš„æµ‹è¯•æ–‡æœ¬
    test_strings = [
        "cafÃ©",  # e + ç»„åˆé‡éŸ³ç¬¦
        "cafÃ©",  # é¢„ç»„åˆçš„Ã©
        "â‘ â‘¡â‘¢",  # åœ†åœˆæ•°å­—
        "ï¬le",  # è¿å­—ç¬¦
    ]

    for test_str in test_strings:
        print(f"\nåŸå§‹æ–‡æœ¬: '{test_str}' (é•¿åº¦: {len(test_str)})")

        comparison = normalizer.compare_normalizations(test_str)

        for form, result in comparison.items():
            equal_mark = "âœ“" if result["text"] == test_str else "âœ—"
            print(
                f"  {form}: '{result['text']}' (é•¿åº¦: {result['length']}, å­—èŠ‚: {result['bytes']}) {equal_mark}"
            )

    # è¾“å‡ºç¤ºä¾‹:
    # åŸå§‹æ–‡æœ¬: 'cafÃ©' (é•¿åº¦: 4)
    #   NFC: 'cafÃ©' (é•¿åº¦: 4, å­—èŠ‚: 5) âœ“
    #   NFD: 'cafÃ©' (é•¿åº¦: 5, å­—èŠ‚: 6) âœ—
    #   NFKC: 'cafÃ©' (é•¿åº¦: 4, å­—èŠ‚: 5) âœ“
    #   NFKD: 'cafÃ©' (é•¿åº¦: 5, å­—èŠ‚: 6) âœ—
    # æµ‹è¯•ä¸åŒç±»å‹çš„å­—ç¬¦
    test_text = "Helloä¸–ç•Œ123ï¼"
    results = analyze_character_properties(test_text)

    for result in results:
        print(f"å­—ç¬¦: '{result['character']}' - åç§°: {result['name']}")
        print(f"  åˆ†ç±»: {result['category']}")
        if result["numeric_value"] is not None:
            print(f"  æ•°å€¼: {result['numeric_value']}")
        print("---")

    # è¾“å‡ºç¤ºä¾‹:
    # å­—ç¬¦: 'H' - åç§°: LATIN CAPITAL LETTER H
    #   åˆ†ç±»: Lu
    # ---
    # å­—ç¬¦: 'ä¸–' - åç§°: CJK UNIFIED IDEOGRAPH-4E16
    #   åˆ†ç±»: Lo
    # ---
    # å­—ç¬¦: '1' - åç§°: DIGIT ONE
    #   åˆ†ç±»: Nd
    #   æ•°å€¼: 1.0
    # ä½¿ç”¨å­—ç¬¦åˆ†æå™¨
    analyzer = UnicodeCharacterAnalyzer()

    sample_text = "Hello, ä¸–ç•Œï¼123 $100 Î±+Î²=Î³ ğŸš€"
    category_counts, detailed_info = analyzer.categorize_text(sample_text)

    print("å­—ç¬¦åˆ†ç±»ç»Ÿè®¡:")
    for category, count in sorted(category_counts.items()):
        description = analyzer.category_descriptions.get(category, "æœªçŸ¥")
        print(f"  {category} ({description}): {count}ä¸ª")

    print("\nä»…ä¿ç•™å­—æ¯å’Œæ•°å­—:")
    letters_and_digits = analyzer.filter_by_category(
        sample_text, ["Lu", "Ll", "Lo", "Nd"]
    )
    print(f"åŸæ–‡: {sample_text}")
    print(f"è¿‡æ»¤å: {letters_and_digits}")

    # è¾“å‡ºç¤ºä¾‹:
    # å­—ç¬¦åˆ†ç±»ç»Ÿè®¡:
    #   Ll (å°å†™å­—æ¯): 4ä¸ª
    #   Lo (å…¶ä»–å­—æ¯): 2ä¸ª
    #   Lu (å¤§å†™å­—æ¯): 1ä¸ª
    #   Nd (åè¿›åˆ¶æ•°å­—): 3ä¸ª
    #   Po (å…¶ä»–æ ‡ç‚¹): 2ä¸ª
    #   ...
    #
    # ä»…ä¿ç•™å­—æ¯å’Œæ•°å­—:
    # åŸæ–‡: Hello, ä¸–ç•Œï¼123 $100 Î±+Î²=Î³ ğŸš€
    # è¿‡æ»¤å: Helloä¸–ç•Œ123100Î±Î²Î³